[[alerts-ui-monitor]]
== Monitor and troubleshoot rule executions

:frontmatter-description: Find out how your rules are performing, and troubleshoot common rule issues.
:frontmatter-tags-products: [security]
:frontmatter-tags-content-type: [how-to]
:frontmatter-tags-user-goals: [monitor, manage]

Several tools can help you gain insight into the performance of your detection rules:

* <<rule-monitoring-tab, Rule Monitoring tab>> — The current state of all detection rules and their most recent executions. Go to the *Rule Monitoring* tab to get an overview of which rules are running, how long they're taking, and if they're having any trouble.

* <<rule-execution-logs, Execution results>> — Historical data for a single detection rule's executions over time. Consult the execution results to understand how a particular rule is running and whether it's creating the alerts you expect.

* <<rule-monitoring-dashboard, Detection rule monitoring dashboard>> — Visualizations to help you monitor the overall health and performance of {elastic-sec}'s detection rules. Consult this dashboard for a high-level view of whether your rules are running successfully and how long they're taking to run, search data, and create alerts.

Refer to the <<troubleshoot-signals>> section below for strategies on adjusting rules if they aren't creating the expected alerts.

[float]
[[rule-monitoring-tab]]
=== Rule Monitoring tab

To view a summary of all rule executions (including the most recent failures, execution times, and gaps in rule executions), select the *Rule Monitoring* tab on the *Rules* page. To access the tab, find **Detection rules (SIEM)** in the navigation menu or look for “Detection rules (SIEM)” using the {kibana-ref}/introduction.html#kibana-navigation-search[global search field], then go to the *Rule Monitoring* tab. 

[role="screenshot"]
image::images/monitor-table.png[]

On the *Rule Monitoring* tab, you can <<sort-filter-rules, sort and filter rules>> just like you can on the *Installed Rules* tab. 

TIP: To sort the rules list, click any column header. To sort in descending order, click the column header again.

For detailed information on a rule, the alerts it generated, and associated errors, click on its name in the table. This also allows you to perform the same actions that are available on the <<rules-ui-management, **Installed Rules** tab>>, such as modifying or deleting rules, activating or deactivating rules, exporting or importing rules, and duplicating prebuilt rules.

For information about rule execution gaps (which are periods of time when a rule didn't run), use the panel above the table. The time filter on the left allows you to select a time range for viewing gap data. The **Total rules with gaps:** field tells you how many rules have unfilled or partially filled gaps within the selected time range. The **Only rules with gaps** filter on the right lets you only display rules with unfilled or partially filled gaps. 

Within the table, the **Last Gap (if any)** column conveys how long the most recent gap for a rule lasted. The **Unfilled gaps duration** column shows whether a rule still has gaps and provides a total sum of the remaining unfilled or partially filled gaps. If a rule has no gaps, these columns display a dash (`––`).

TIP: For a detailed view of a rule's gaps, go to the **Execution results** tab and check the <<gaps-table>>.

[float]
[[rule-execution-logs]]
=== Execution results tab

From the **Execution results** tab, you can access the rule’s execution log, monitor and address gaps in a rule's execution schedule, and check manual runs for the rule. To find the tab, click the rule's name to open its details, then scroll down. 

[float]
[[execution-results-tab]]
==== Execution log table

Each detection rule execution is logged, including the execution type, the execution's success or failure, any warning or error messages, how long it took to search for data, create alerts, and complete. This can help you troubleshoot a particular rule if it isn't behaving as expected (for example, if it isn't creating alerts or takes a long time to run).

[role="screenshot"]
image::images/rule-execution-logs.png[Execution log table on the rule execution results tab]

You can hover over each column heading to display a tooltip about that column's data. Click a column heading to sort the table by that column. Within the Execution log table, you can click the arrow at the end of a row to expand a long warning or error message.

Use these controls to filter what's included in the logs table:

* The **Run type** drop-down filters the table by rule execution type: 
** **Scheduled**: Automatic, scheduled rule executions.
** **Manual**: Rule executions that were <<manually-run-rules,started manually>>.

* The *Status* drop-down filters the table by rule execution status: 
** *Succeeded*: The rule completed its defined search. This doesn't necessarily mean it generated an alert, just that it ran without error.
** *Failed*: The rule encountered an error that prevented it from running. For example, a {ml} rule whose corresponding {ml} job wasn't running.
** *Warning*: Nothing prevented the rule from running, but it might have returned unexpected results. For example, a custom query rule tried to search an index pattern that couldn't be found in {es}.

* The date and time picker sets the time range of rule executions included in the table. This is separate from the global date and time picker at the top of the rule details page.

* The **Source event time range** button toggles the display of data pertaining to the time range of manual runs.

* The *Show metrics columns* toggle includes more or less data in the table, pertaining to the timing of each rule execution.

* The *Actions* column allows you to show alerts generated from a given rule execution. Click the filter icon (image:images/filter-icon.png[Filter icon,18,17]) to create a global search filter based on the rule execution's ID value. This replaces any previously applied filters, changes the global date and time range to 24 hours before and after the rule execution, and displays a confirmation notification. You can revert this action by clicking *Restore previous filters* in the notification.

[float]
[[gaps-table]]
==== Gaps table

beta::[]

Gaps in rule executions are periods of time where a rule didn’t run. Gaps can be caused by various disruptions, including system updates, rule failures, or simply turning off a rule. Addressing them is essential for maintaining consistent coverage and avoiding missed alerts. 

TIP: Refer to the <<troubleshoot-gaps>> section for strategies for avoiding gaps.

Use the information in the Gaps table to assess the scope and severity of rule execution gaps. To control what's shown in the table, you can filter the table by gap status, select a time range for viewing gap data, and sort multiple columns. 

[role="screenshot"]
image::images/gaps-table.png[Gaps table on the rule execution results tab]

The Gaps table has the following columns:

* **Status**: The current state of the gap. It can be `Filled`, `Partially filled`, or `Unfilled`.
* **Detected at**: The date and time the gap was first discovered.
* **Manual fill tasks**: The status of the manual run that’s filling the gap. For more details about the manual run, refer to its entry in the <<manual-runs-table,manual run table>>.
* **Event time covered**: How much progress the manual run has made filling the gap. 
+
NOTE: If you stop a manual run that hasn't finished filling a gap, the gap’s status will be set to `Partially filled`. To fill the remaining gap, you can select the **Fill remaining gap** action or <<manually-run-rules,manually run>> the rule over the gap's time frame.
+
* **Range**: When the gap started and ended. 
* **Total gap duration**: How long the gap lasted.
* **Actions**: The actions that you can take for the gap. They can be **Fill gap** (starts a manual run to fill the gap) or **Fill remaining gap** (starts a manual run that fills the leftover portion of the gap).

[float]
[[manual-runs-table]]
==== Manual runs table

You can <<manually-run-rules,manually run>> enabled rules for a specified period of time to force test them, provide additional rule coverage, or fill gaps in rule executions. Each manual run can produce multiple rule executions, depending on the time range of the run and the rule's execution schedule. 

NOTE: Manual runs are executed with low priority and limited concurrency, meaning they might take longer to complete. This can be especially apparent for rules requiring multiple executions.

The Manual runs table tracks manual rule executions and provides important details such as:

* The total number of rule executions that the manual run will produce and how many are failing, pending, running, and completed.
* When the manual run started and the time range that it will cover.
+
NOTE: To stop an active run, go to the appropriate row in the table and click **Stop run** in the **Actions** column. Completed rule executions for each manual run are logged in the Execution log table.
+
* The status of each manual run:
** `Pending`: The rule is not yet running. 
** `Running`: The rule is executing during the time range you specified. Some rule types, such as indicator match rules, can take longer to run.
** `Error`: The rule's configuration is preventing it from running correctly. For example, the rule's conditions cannot be validated.

[role="screenshot"]
image::images/manual-rule-run-table.png[Manual rule runs table on the rule execution results tab]


[float]
[[troubleshoot-signals]]
=== Troubleshoot missing alerts

When a rule fails to run close to its scheduled time, some alerts may be
missing. There are a number of ways to try to resolve this issue:

* <<troubleshoot-gaps>>
* <<troubleshoot-ingestion-pipeline-delay>>
* <<ml-job-compatibility>>

You can also use Task Manager in {kib} to troubleshoot background tasks and processes that may be related to missing alerts:

* {kibana-ref}/task-manager-health-monitoring.html[Task Manager health monitoring]
* {kibana-ref}/task-manager-troubleshooting.html[Task Manager troubleshooting]

[float]
[[troubleshoot-max-alerts]]
==== Troubleshoot maximum alerts warning

When a rule reaches the maximum number of alerts it can generate during a single rule execution, the following warning appears on the rule's details page and in the rule execution log: `This rule reached the maximum alert limit for the rule execution. Some alerts were not created.` 

If you receive this warning, go to the rule's **Alerts** tab and check for anything unexpected. Unexpected alerts might be created from data source issues or queries that are too broadly scoped. To further reduce alert volume, you can also add <<add-exceptions,rule exceptions>> or <<alert-suppression,suppress alerts>>. 

[float]
[[troubleshoot-gaps]]
==== Troubleshoot gaps

If you see values in the Gaps column in the Rule Monitoring table or on the Rule details page
for a small number of rules, you can edit those rules and increase their additional look-back time.

It's recommended to set the `Additional look-back time` to at
least 1 minute. This ensures there are no missing alerts when a rule doesn't
run exactly at its scheduled time.

{elastic-sec} prevents duplication. Any duplicate alerts that are discovered during the
`Additional look-back time` are _not_ created.

NOTE: If the rule that experiences gaps is an indicator match rule, see <<tune-indicator-rules, how to tune indicator match rules>>. Also please note that {elastic-sec} provides <<support-indicator-rules, limited support for indicator match rules>>.

If you see gaps for numerous rules:

* If you restarted {kib} when many rules were activated, try deactivating them
and then reactivating them in small batches at staggered intervals. This
ensures {kib} does not attempt to run all the rules at the same time.
* Consider adding another {kib} instance to your environment.

[float]
[[troubleshoot-ingestion-pipeline-delay]]
==== Troubleshoot ingestion pipeline delay

Even if your rule runs at its scheduled time, there might still be missing alerts if your ingestion pipeline delay is greater than your rule interval + additional look-back time. Prebuilt rules have a minimum interval + additional look-back time of 6 minutes in {stack} version >=7.11.0. To avoid missed alerts for prebuilt rules, use caution to ensure that ingestion pipeline delays remain below 6 minutes.

In addition, use caution when creating custom rule schedules to ensure that the specified interval + additional look-back time is greater than your deployment's ingestion pipeline delay.

You can reduce the number of missed alerts due to ingestion pipeline delay by specifying the `Timestamp override` field value to `event.ingested` in <<rule-ui-advanced-params, advanced settings>> during rule creation or editing. The detection engine uses the value from the `event.ingested` field as the timestamp when executing the rule.

For example, say an event occurred at 10:00 but wasn't ingested into {es} until 10:10 due to an ingestion pipeline delay. If you created a rule to detect that event with an interval + additional look-back time of 6 minutes, and the rule executes at 10:12, it would still detect the event because the `event.ingested` timestamp was from 10:10, only 2 minutes before the rule executed and well within the rule's 6-minute interval + additional look-back time.

[role="screenshot"]
image::images/timestamp-override.png[]

[float]
[[ml-job-compatibility]]
==== Troubleshoot missing alerts for {ml} jobs

{ml-cap} detection rules use {ml} jobs that have dependencies on data fields populated by the {beats} and {agent} integrations. In {stack} version 8.3, new {ml} jobs (prefixed with `v3`) were released to operate on the ECS fields available at that time. 

If you're using 8.2 or earlier versions of {beats} or {agent} with {stack} version 8.3 or later, you may need to duplicate prebuilt rules or create new custom rules _before_ you update the Elastic prebuilt rules. Once you update the prebuilt rules, they will only use `v3` {ml} jobs. Duplicating the relevant prebuilt rules before updating them ensures continued coverage by allowing you to keep using `v1` or `v2` jobs (in the duplicated rules) while also running the new `v3` jobs (in the updated prebuilt rules).

[IMPORTANT]
=====
* Duplicated rules may result in duplicate anomaly detections and alerts.
* Ensure that the relevant `v3` {ml} jobs are running before you update the Elastic prebuilt rules.
=====

* If you only have *8.3 or later versions of {beats} and {agent}*: You can download or update your prebuilt rules and use the latest `v3` {ml} jobs. No additional action is required.

* If you only have *8.2 or earlier versions of {beats} or {agent}*, or *a mix of old and new versions*: To continue using the `v1` and `v2` {ml} jobs specified by pre-8.3 prebuilt detection rules, you must duplicate affected prebuilt rules _before_ updating them to the latest rule versions. The duplicated rules can continue using the same `v1` and `v2` {ml} jobs, and the updated prebuilt {ml} rules will use the new `v3` {ml} jobs.

* If you have *a non-Elastic data shipper that gathers ECS-compatible events*: You can use the latest `v3` {ml} jobs with no additional action required, as long as your data shipper uses the latest ECS specifications. However, if you're migrating from {ml} rules using `v1`/`v2` jobs, ensure that you start the relevant `v3` jobs before updating the Elastic prebuilt rules.

The following Elastic prebuilt rules use the new `v3` {ml} jobs to generate alerts. Duplicate their associated `v1`/`v2` prebuilt rules _before_ updating them if you need continued coverage from the `v1`/`v2` {ml} jobs:

* <<unusual-linux-network-port-activity>>: `v3_linux_anomalous_network_port_activity`

* <<unusual-linux-network-connection-discovery>>: `v3_linux_anomalous_network_connection_discovery`

* <<anomalous-process-for-a-linux-population>>: `v3_linux_anomalous_process_all_hosts`

* <<unusual-linux-username>>: `v3_linux_anomalous_user_name`

* <<unusual-linux-process-calling-the-metadata-service>>: `v3_linux_rare_metadata_process`

* <<unusual-linux-user-calling-the-metadata-service>>: `v3_linux_rare_metadata_user`

* <<unusual-process-for-a-linux-host>>: `v3_rare_process_by_host_linux`

* <<unusual-process-for-a-windows-host>>: `v3_rare_process_by_host_windows`

* <<unusual-windows-network-activity>>: `v3_windows_anomalous_network_activity`

* <<unusual-windows-path-activity>>: `v3_windows_anomalous_path_activity`

* <<anomalous-windows-process-creation>>: `v3_windows_anomalous_process_creation`

* <<anomalous-process-for-a-windows-population>>: `v3_windows_anomalous_process_all_hosts` 

* <<unusual-windows-username>>: `v3_windows_anomalous_user_name`

* <<unusual-windows-process-calling-the-metadata-service>>: `v3_windows_rare_metadata_process`

* <<unusual-windows-user-calling-the-metadata-service>>: `v3_windows_rare_metadata_user`
