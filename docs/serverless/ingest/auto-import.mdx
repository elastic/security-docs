---
slug: /serverless/security/auto-import
title: Automatic import
description: Use auto import to quickly ingest third-party data.
tags: [ 'serverless', 'security', 'how-to' ]
status: in review
---

<DocBadge template="technical preview" />

<DocCallOut title="Technical preview" color="warning">
This feature is in technical preview. It may change in the future, and you should exercise caution when using it in production environments. Elastic will work to fix any issues, but features in technical preview are not subject to the support SLA of GA features.
</DocCallOut>

This page explains how to use automatic import. Automatic import helps you quickly parse, ingest, and create [ECS mappings](https://www.elastic.co/elasticsearch/common-schema) for data from sources that don't yet have prebuilt Elastic integrations. This can accelerate your migration to Elastic's SIEM, and help you quickly add new data sources to an existing SIEM deployment.

Automatic import uses a large language model (LLM) with specialized instructions to quickly analyze your source data and create a custom integration. 

<DocCallOut title="Requirements">

- A working <DocLink slug="/serverless/security/connect-to-bedrock" text="Amazon Bedrock connector "/>.
- A [security analytics complete](https://www.elastic.co/pricing/serverless-security) subscription.
- A sample of the data you want to import, in JSON format. 

</DocCallOut>

<DocCallOut title="Recommended models">
Automatic import currently works with all variants of Claude 3. Other models are not currently supported.
</DocCallOut>


## Create a new custom integration

1. In your ((elastic-sec)) deployment, click **Add integrations**.
2. Under **Can't find an integration?** click **Create new integration**.
3. Click **Create integration**.
4. Select an <DocLink slug="/serverless/security/connect-to-bedrock" text="Amazon Bedrock connector"/>. 
5. Define how your new integration will appear on the integrations page by providing a **Title**, **Description**, and **Logo**.  Click **Next**.
6. Define your integration's package name, which will prefix the imported event fields. 
7. Define your **Data stream title**, **Data stream description**, and **Data stream name**. These fields will appear on the integration's configuration page to help identify the data stream it writes to. For more information, refer to [Elastic data stream naming scheme](https://www.elastic.co/blog/an-introduction-to-the-elastic-data-stream-naming-scheme).
8. Select your [**Data collection method**](https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-options.html). 
9. Upload a sample of your data in JSON format. Make sure to include all the types of events that you want the new integration to handle. Only the first ten events in the sample are analyzed; additional data is truncated.
10. Click **Analyze logs**, then wait for processing to complete. This may take several minutes.
11. After processing is complete, the pipeline's field mappings appear, including both ECS and custom fields. \[Screenshot\]
12. (Optional) After reviewing the proposed pipeline, you can finetune it by clicking **Edit pipeline**. \[Screenshot\]. When you're satisfied with your changes, click **Save**. 
13. Click **Add to Elastic**. After the **Success** message appears, your new integration will be available on the **Integrations** page. \[Screenshot\] 

Click **Add to an agent** to deploy your new integration and start collecting data, or click **View integration** to view detailed information about your new integration. 

<DocCallOut title="Note">
Once you've added an integration, it is read-only other than the ingest pipeline, which you can edit by going to **Stack management â†’ Ingest pipelines**. Other characteristics such as the integration's title, description, and logo cannot be edited after creation.
</DocCallOut>

<DocCallOut title="Important">
Insert legal disclaimer
</DocCallOut>




